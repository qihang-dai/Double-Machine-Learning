{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Homework 4 - Spring 2023**\n",
        "\n",
        "**Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. This is the master notebook so <u>you will not be able to save your changes without copying it </u>! Once you click on that, make sure you are working on that version of the notebook so that your work is saved** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCqibYyjERm_"
      },
      "outputs": [],
      "source": [
        "# Restart the runtime after running this cell everytime you open the notebook\n",
        "!pip install pandas==1.1.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill"
      ],
      "metadata": {
        "id": "A4Wy5y623XUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "from scipy.spatial import distance\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "np.random.seed(42)  # don't change this line\n",
        "\n",
        "import base64\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2VtEzsZ-loR"
      },
      "outputs": [],
      "source": [
        "# For autogreader only, do not modify this cell. \n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**. \n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install penngrader --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = ...          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immediately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_QDnZk-vvI"
      },
      "outputs": [],
      "source": [
        "#GRADER TODO\n",
        "grader = PennGrader(homework_id = 'CIS5190_Sp23_HW4', student_id = STUDENT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0_ydbgD0Kvf"
      },
      "outputs": [],
      "source": [
        "# Serialization code needed by the autograder\n",
        "import inspect, sys\n",
        "from IPython.core.magics.code import extract_symbols\n",
        "\n",
        "def new_getfile(object, _old_getfile=inspect.getfile):\n",
        "    if not inspect.isclass(object):\n",
        "        return _old_getfile(object)\n",
        "    \n",
        "    # Lookup by parent module (as in current inspect)\n",
        "    if hasattr(object, '__module__'):\n",
        "        object_ = sys.modules.get(object.__module__)\n",
        "        if hasattr(object_, '__file__'):\n",
        "            return object_.__file__\n",
        "    \n",
        "    # If parent module is __main__, lookup by methods (NEW)\n",
        "    for name, member in inspect.getmembers(object):\n",
        "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
        "            return inspect.getfile(member)\n",
        "    else:\n",
        "        raise TypeError('Source for {!r} not found'.format(object))\n",
        "inspect.getfile = new_getfile\n",
        "\n",
        "def grader_serialize(obj):\n",
        "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
        "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
        "    return class_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXK6ET2npr2"
      },
      "source": [
        "## Datasets\n",
        "Next, we will download the dataset from Google Drive to your local runtime. After successful download, you may verify that all datasets are present in your colab instance.\n",
        "\n",
        "- [observations.csv](https://drive.google.com/file/d/1RvNTrL147Cx90ABv4IfXcexaRyHB-U-e/view?usp=sharing)\n",
        "- [test_student.csv](https://drive.google.com/file/d/1EjQ3Jy5q25GaxeNKh4ahtsLgHEyW3tUj/view?usp=sharing)\n",
        "\n",
        "\n",
        "#### Acknowledgement\n",
        "Dataset obtained from kaggle.com [Hourly Weather Surface - Brazil (Southeast region)](https://www.kaggle.com/PROPPG-PPG/hourly-weather-surface-brazil-southeast-region/metadata )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dexqdaw4n3Yo"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "  import gdown\n",
        "  if not os.path.exists(\"observations.csv\"):\n",
        "    !gdown --id 1RvNTrL147Cx90ABv4IfXcexaRyHB-U-e\n",
        "  if not os.path.exists(\"test_student.csv\"):\n",
        "    !gdown --id 1Z0I6iylDgTk2OKuKDaQVR9I1aJvgRsn_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdHHkKWlXSX"
      },
      "source": [
        "#### **NOTE 1. If you are running into a `__builtins__' error, it's likely because you're using a function call of the form numpy.ndarray.mean(), like a.mean(). This does not play nice with PennGrader unfortunately. Please use the function call numpy.mean(a) instead.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "502P2SnfQjjB"
      },
      "outputs": [],
      "source": [
        "def prepare_final_cleaned_df(df):\n",
        "  df = df.drop([\"Unnamed: 0\"], axis=1)\n",
        "  df[\"mdct\"] = pd.to_datetime(df[\"mdct\"])\n",
        "  df.loc[df[\"gust\"].isna(),\"gust\"] = 0\n",
        "  df.loc[df[\"gbrd\"].isna(),\"gbrd\"] = 0\n",
        "  df.loc[df[\"wdsp\"].isna(),\"wdsp\"] = 0\n",
        "  df.loc[df[\"dewp\"].isna(),\"dewp\"] = 0\n",
        "  df.loc[df[\"dmin\"].isna(),\"dmin\"] = 0\n",
        "  df.loc[df[\"dmax\"].isna(),\"dmax\"] = 0\n",
        "  df = df[df[\"temp\"] != 0]\n",
        "  df = df.drop(columns=[\"prcp\"])\n",
        "\n",
        "  left_df = df.copy()\n",
        "  right_df = df.copy()\n",
        "  right_df[\"mdct\"] = right_df[\"mdct\"].apply(lambda x : x + datetime.timedelta(hours=1))\n",
        "\n",
        "  columns = [\"stp\", \"smax\", \"smin\", \"gbrd\", \"dewp\", \"tmax\", \n",
        "            \"dmax\", \"tmin\", \"dmin\", \"hmdy\", \"hmax\",\n",
        "            \"hmin\", \"wdsp\", \"wdct\", \"gust\", \"temp\"]\n",
        "          \n",
        "  merged_df = pd.merge(left_df, right_df, \"left\", on=[\"wsid\",\"mdct\"], indicator=True)\n",
        "  merged_df = merged_df[merged_df['_merge'] == \"both\"]\n",
        "\n",
        "  columns_x = [x + \"_x\" for x in columns]\n",
        "  columns_y = [x + \"_y\" for x in columns]\n",
        "  \n",
        "  merged_df[columns] = merged_df[columns_x].values - merged_df[columns_y].values\n",
        "  merged_df = merged_df.drop(columns=columns_x)\n",
        "  merged_df = merged_df.drop(columns=columns_y)\n",
        "  merged_df = merged_df.drop(columns=[\"_merge\", \"mdct\", \"wsid\"])\n",
        "\n",
        "  final_cleaned_df = merged_df.copy()\n",
        "\n",
        "  final_cleaned_df.loc[final_cleaned_df[\"temp\"] >= 0, \"temp\" ] = 1\n",
        "  final_cleaned_df.loc[final_cleaned_df[\"temp\"] < 0, \"temp\" ] = 0\n",
        "  return final_cleaned_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz_bGjWTOewG"
      },
      "source": [
        "# **1. [25 pts, autograded] AdaBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYgUBn3BX9ol"
      },
      "source": [
        "## **1.1.  [3 pts] Logistic regression with sample weights**\n",
        "\n",
        "As you will have learnt from the lectures, AdaBoost fits weak learners (here, logistic regression model)  in each iteration, to a dataset with weights $w_i$ attached to each sample $(x_i, y_i)$. The loss function now becomes:\n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N w_{i} \\times [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
        "$\n",
        "\n",
        "where $h_\\theta(x)$ is the logistic regression hypothesis function.\n",
        "\n",
        "The gradient of this weighted loss function with respect to the weight $\\theta_j$ is given by:\n",
        "\n",
        "> $\\frac{\\partial \\mathcal{L}({\\theta})}{\\partial \\theta_j} = \\sum_{i=1}^{N}w_{i}(h_{{\\theta}}({x}_i) - y_i)x_{ij}$\n",
        "\n",
        "Using this information, complete the `compute_gradient` method in the `LogisticRegression` class to account for sample weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1PEAbJ7-Q4r"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.1\n",
        "        Learning rate\n",
        "    tol : float, default=0.01\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=1000\n",
        "        Maximum number of iterations of gradient descent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    converged_: boolean\n",
        "        Boolean value indicating whether gradient descent converged or not\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.1, tol=0.01, max_iter=1000):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.converged_ = False\n",
        "\n",
        "    def compute_gradient(self, theta, X, y, sample_weight):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "        sample_weight: numpy.ndarray of shape (N,)\n",
        "            The sample weight array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "        y_hat = sigmoid(X.dot(theta))\n",
        "\n",
        "        # STUDENT TODO: Compute the gradient\n",
        "        ...\n",
        "        \n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def fit(self, X, y, sample_weight):\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "        sample_weight: numpy.ndarray of shape (N,)\n",
        "            The sample weight array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "\n",
        "        # Initializing the weights\n",
        "        theta_old = np.zeros((D + 1,))\n",
        "        theta_new = theta_old.copy()\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            theta_new = theta_old - self.alpha * self.compute_gradient(theta_old, X, y, sample_weight)\n",
        "\n",
        "            if np.linalg.norm(theta_new - theta_old) / (np.linalg.norm(theta_old) + self.tol) <= self.tol:\n",
        "                self.converged_ = True\n",
        "                break\n",
        "            \n",
        "            theta_old = theta_new.copy()\n",
        "\n",
        "        self.theta_ = theta_new\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probabilities that the data points in X belong to class 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted probabilities that the data points in X belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        \n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "\n",
        "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "        y_hat = sigmoid(X.dot(self.theta_))\n",
        "        return y_hat\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        y_hat = self.predict_proba(X)\n",
        "        y_pred = y_hat.copy()\n",
        "        y_pred[y_pred >= 0.5] = 1\n",
        "        y_pred[y_pred < 0.5] = 0\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMnnQw5BVv-J"
      },
      "source": [
        "### Test case for the `compute_gradient` method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_compute_gradient(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    np.random.seed(0)\n",
        "    theta_tc = np.random.randn(2)\n",
        "    X_tc = np.random.randn(100, 2)\n",
        "    y_tc = np.random.randint(0, 2, 100)\n",
        "    sample_weight_tc = np.random.uniform(0, 1, 100)\n",
        "    student_ans = student_lr_clf.compute_gradient(theta_tc, X_tc, y_tc, sample_weight_tc)\n",
        "    required_ans = np.array([12.903225675830651, -1.0829605960182223])\n",
        "    \n",
        "    assert np.linalg.norm(student_ans - required_ans) < 1e-2 * required_ans.size\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_compute_gradient(LogisticRegression)"
      ],
      "metadata": {
        "id": "uxm5ieblL_sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNfmQsI99Rgz"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_compute_gradient_autograder', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTuNHGX35ulA"
      },
      "source": [
        "## **1.2. [20 pts] AdaBoostClassifier Implementation**\n",
        "\n",
        "In this section, you will be implementing the AdaBoost classifier with the logistic regression weak learner from above.\n",
        "\n",
        "### **1.2.1. [12 pts] Follow the hints in the `fit` method in the `AdaBoostClassifier` class to implement the following algorithm.**\n",
        "\n",
        "Use the following Adaboost pseudocode as a reference.\n",
        "\n",
        "**INPUT:**\n",
        "\n",
        "1. training data $X, y = \\{(x_{i}, y_{i})\\}_{i=1}^N$\n",
        "\n",
        "2. number of iterations $T$\n",
        "\n",
        "**ALGORITHM:**\n",
        "\n",
        "1.   Initialize $N$ uniform weights, i.e., $w_{1} = [1/N, 1/N, ..., 1/N]$\n",
        "\n",
        "2.   `For` $t = 1, 2, ... T$:\n",
        "\n",
        "> 2.1. Train model $h_t$ on $X$ and $y$ with instance weights $w_{t}$\n",
        "\n",
        "> 2.2. Compute the weighted training error rate of $h_{t}$: $\\epsilon_{t} = \\sum_{i: y_i \\ne h_t(x_i)} w_{t,i}$\n",
        "\n",
        "> 2.3. If $\\epsilon_{t} > 0.5$, flip $h_t$'s predictions\n",
        "\n",
        "> 2.4. Set $\\beta_{t} = \\frac{1}{2}\\text{ln}\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$\n",
        "\n",
        "> 2.5. Update all instance weights: $w_{t + 1,i} = w_{t,i}\\times\\text{exp}(-\\beta_{t}y_{i}h_{t}(x_{i}))$ $\\forall i = 1, 2, ..., N$\n",
        "\n",
        "> 2.6. Normalize $w_{t+1}$ such that the elements sum to 1\n",
        "\n",
        "> `End For`\n",
        "\n",
        "### **1.2.2. [8 pts] Follow the hints in the `predict` method in the `AdaBoostClassifier` class for obtaining the predictions of the trained AdaBoost classifier.**\n",
        "\n",
        "> $H(x) = \\text{sign}\\left(\\sum_{t=1}^{T}\\beta_{t}h_{t}(x)\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-ep_MeohVZM"
      },
      "outputs": [],
      "source": [
        "class AdaBoostClassifier:\n",
        "    \"\"\"\n",
        "    AdaBoost classifier based on logistic regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    T: int, default=100\n",
        "        The number of logistic regression models in the boosting model\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    beta_arr_ : list of length T\n",
        "        The list of beta values in the boosting model\n",
        "\n",
        "    h_arr_: list of length T\n",
        "        The list of logistic regression models in the boosting model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, T=100):\n",
        "\n",
        "        self.T = T\n",
        "\n",
        "        self.beta_arr_ = []\n",
        "        self.h_arr_ = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the logistic regression models (h) and compute their coefficients (beta), \n",
        "        and store them in h_arr_ and beta_arr_ respectively.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "\n",
        "        # STUDENT TODO: Initialize w with appropriate values\n",
        "        ...\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        y_ = y.copy()\n",
        "        # STUDENT TODO: Update y_ such that the 0's in y_ are replaced by -1\n",
        "        ...\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        for t in range(self.T):\n",
        "            h = LogisticRegression()\n",
        "\n",
        "            # STUDENT TODO: \n",
        "            # Fit h to X and y using w as the sample weights\n",
        "\n",
        "            # Obtain the predictions from h and compute epsilon\n",
        "\n",
        "            # If epsilon > 0.5:\n",
        "            # 1. flip the predictions, i.e., replace 1's with 0's and 0's with 1's\n",
        "            # 2. invert the model (h), i.e., make it predict 1 for what it predicted 0 earlier and vice-versa (clue: think about modifying h.theta_)        \n",
        "\n",
        "            # STUDENT TODO END\n",
        "\n",
        "            self.h_arr_.append(h)\n",
        "\n",
        "            if epsilon == 0:\n",
        "                beta = np.inf\n",
        "                self.beta_arr_.append(beta)\n",
        "                break\n",
        "            \n",
        "            # STUDENT TODO: Compute beta\n",
        "            ...\n",
        "            # STUDENT TODO END\n",
        "\n",
        "            self.beta_arr_.append(beta)\n",
        "            y_pred_ = y_pred.copy()\n",
        "\n",
        "            # STUDENT TODO: \n",
        "            # Update y_pred_ such that the 0's in y_pred_ are replaced by -1\n",
        "\n",
        "            # Update w and normalize it such that the values in w sum to 1\n",
        "\n",
        "            # STUDENT TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "        \n",
        "        N = X.shape[0]\n",
        "        \n",
        "        # Initialize the summation of beta times h for each x_i \n",
        "        sum_beta_times_h = np.zeros((N,))\n",
        "\n",
        "        for t in range(len(self.h_arr_)):\n",
        "            \n",
        "            # STUDENT TODO: \n",
        "            # Obtain the predictions of the t-th model in self.h_arr_\n",
        "\n",
        "            # Replace the 0's in the array with -1\n",
        "\n",
        "            # Update sum_beta_times_h\n",
        "\n",
        "\n",
        "        # Create an array `y_pred` for the final predictions\n",
        "        # Fill 0's and 1's in `y_pred` depending on the sum_beta_time_h value in the corresponding location\n",
        "\n",
        "        return y_pred\n",
        "        # STUDENT TODO END\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EICaCxuHUY2j"
      },
      "source": [
        "### Test case for the `fit` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w30orkoA8w5A"
      },
      "outputs": [],
      "source": [
        "def test_adaboost_fit(StudentAdaBoostClassifier):\n",
        "\n",
        "    T = 4\n",
        "    N = 100\n",
        "    D = 2\n",
        "\n",
        "    student_ab_clf = StudentAdaBoostClassifier(T=T)\n",
        "    np.random.seed(0)\n",
        "    X_tc = np.random.randn(N, D)\n",
        "    y_tc = np.random.randint(0, 2, N)\n",
        "    student_ab_clf.fit(X_tc, y_tc)\n",
        "\n",
        "    beta_arr_student_ans = student_ab_clf.beta_arr_\n",
        "    beta_arr_required_ans = np.array([0.08017132503758954, 0.046732864002838985, \n",
        "                                      0.022808008179707476, 0.07012335626140642])\n",
        "    assert np.linalg.norm(beta_arr_student_ans - beta_arr_required_ans) < 1e-2 * beta_arr_required_ans.size\n",
        "\n",
        "    h_arr_student_ans = np.zeros([T, D + 1])\n",
        "\n",
        "    for indx, h in enumerate(student_ab_clf.h_arr_):\n",
        "        h_arr_student_ans[indx] = h.theta_\n",
        "\n",
        "    h_arr_required_ans = np.array([[-0.01514967, -0.01713051,  0.21344566],\n",
        "                                   [-0.01738886, -0.00656722,  0.12035635],\n",
        "                                   [-0.0132557,  -0.00428943, 0.06616284],\n",
        "                                   [-0.01037174, -0.00334141,  0.03943088]])\n",
        "\n",
        "    assert np.linalg.norm(h_arr_student_ans - h_arr_required_ans) < 1e-2 * h_arr_required_ans.size\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_adaboost_fit(AdaBoostClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u89ffT3LWtLJ"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_adaboost_fit_autograder', answer = (grader_serialize(LogisticRegression), grader_serialize(AdaBoostClassifier)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6NG8KfQUg1c"
      },
      "source": [
        "### Test case for the `predict` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGB1aPLgWtfT"
      },
      "outputs": [],
      "source": [
        "def test_adaboost_predict(StudentAdaBoostClassifier):\n",
        "\n",
        "    T = 4\n",
        "    N = 100\n",
        "    D = 2\n",
        "\n",
        "    student_ab_clf = StudentAdaBoostClassifier(T=T)\n",
        "    np.random.seed(0)\n",
        "    X_tc = np.random.randn(N, D)\n",
        "    y_tc = np.random.randint(0, 2, N)\n",
        "    student_ab_clf.fit(X_tc, y_tc)\n",
        "\n",
        "    student_ans = student_ab_clf.predict(X_tc)\n",
        "    required_ans = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, \n",
        "                    0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, \n",
        "                    1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, \n",
        "                    0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, \n",
        "                    1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
        "\n",
        "    assert np.mean(student_ans == required_ans) >= 0.98\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_adaboost_predict(AdaBoostClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pixKpOEbXxX"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_adaboost_predict_autograder', answer = (grader_serialize(LogisticRegression), grader_serialize(AdaBoostClassifier)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sFtGKi_WQBm"
      },
      "source": [
        "## **1.3. [2 pts] AdaBoost on the dataset**\n",
        "\n",
        "Follow the hints in the `adaboost_on_dataset` method in the below cell to run `AdaBoostClassifier` on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFiERDRADXBf"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"observations.csv\")\n",
        "final_cleaned_df = prepare_final_cleaned_df(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwXKNBEiYPX3"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    test_df = pd.read_csv(\"test_student.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "def adaboost_on_dataset():\n",
        "    \"\"\"\n",
        "    Trains the AdaBoostClassifier on a real-world dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Nothing\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_test_pred: numpy.ndarray\n",
        "        The predicted classes of the datapoints in test_df\n",
        "    \"\"\"\n",
        "\n",
        "    # STUDENT TODO START: Initialize X_train and y_train with appropriate values (clue: use .iloc followed by .values of the DataFrame class)\n",
        "    ...\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    # STUDENT TODO START: Initialize X_test\n",
        "    ...\n",
        "    # STUDENT TODO END\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    # STUDENT TODO START: Scale the features of X_train and X_test using scaler\n",
        "    ...\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    clf = AdaBoostClassifier(T=10)\n",
        "    # STUDENT TODO START: Now fit clf to the entire training data, i.e., X_train and y_train after feature scaling\n",
        "    ...\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    # STUDENT TODO START: Predict the classes of the datapoints in X_test and return the result\n",
        "    ...\n",
        "    # STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h20BZarho6w_"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    y_test_pred = adaboost_on_dataset()\n",
        "    grader.grade(test_case_id = 'test_adaboost_on_dataset_autograder', answer = y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK4vJh-wDbdC"
      },
      "source": [
        "# **2. [8 pts, 419-optional, autograded] XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdO8f_FWRLt5"
      },
      "source": [
        "## TODOs for this section:\n",
        "- You'll use xgboost library to build a classifier for the above problem. XGBoost is a popular library for gradient boosting, and you can find its documentation [here](https://xgboost.readthedocs.io/en/latest/). \n",
        "- You need to get at least 0.75 accuracy on the test set to receive full credits from the autograder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdOEarsQ8w5L"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "  train_df = final_cleaned_df.copy()\n",
        "  test_df = pd.read_csv(\"test_student.csv\").drop(columns=[\"Unnamed: 0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JimlDZbFDsX0"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    import xgboost as xgb\n",
        "    \n",
        "    # STUDENT TODO STARTS: \n",
        "    # 1. Fit an xgboost classifier to the training data (train_df, target variable is temp) \n",
        "    # 2. Obtain the predictions of the trained model on test_df in the variable y_test_pred\n",
        "    # 3. Tune the hyperparameters such that you pass the autograder threshold accuracy of 0.75\n",
        "    ...\n",
        "    \n",
        "    # STUDENT TODO ENDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGs2eXAYnjK8"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_xgboost', answer = y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDoI9Xz1Dkam"
      },
      "source": [
        "#**3. [25 pts, 15 autograded, 10 manually graded] K-means Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKFPy_KWdWqK"
      },
      "source": [
        "We will implement the k-means clustering algorithm using the Breast Cancer dataset. As with all unsupervised learning problems, our goal is to discover and describe some hidden structure in unlabeled data. The k-means algorithm, in particular, attempts to determine how to separate the data into <em>k</em> distinct groups over a set of features ***given that we know (are provided) the value of k***.\n",
        "\n",
        "Knowing there are <em>k</em> distinct 'classes' however, doesn't tell anything about the content/properties within each class. If we could find samples that are representative of each of these *k* groups, then we could label the rest of the data based on how similar they are to each of the prototypical samples. We will refer to these representatives as the centroids (cluster centers) that correspond to each cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gswBAEyLdYoX"
      },
      "source": [
        "## **3.1. Import the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaQTk5t1daXa"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    from sklearn.datasets import load_breast_cancer\n",
        "    cancer_dataset = load_breast_cancer()\n",
        "\n",
        "    ## TODO your code here ##\n",
        "    \"\"\"\n",
        "    First load the dataset X from cancer_dataset.\n",
        "    X -  (m, n) -> m x n matrix where m is the number of training points = 569 and n is the no of features = 30\n",
        "    \"\"\"\n",
        "    ...\n",
        "    ## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNk3WkQ2dhRp"
      },
      "source": [
        "## **3.2. [12 pts] K-means clustering implementation** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chCjiFFMdjmo"
      },
      "source": [
        "We will first implement a class for k-means clustering.<br>\n",
        "These are the main functions: <br>\n",
        "- `__init__`: The initialiser/constructor (This is implemented for you)\n",
        "- `fit`: Entrypoint function that takes in the dataset (X) as well as centroid initialisations and returns: \n",
        "    - the cluster labels for each row (data point) in the dataset\n",
        "    - list of centroids corresponding to each cluster \n",
        "    - no of iterations taken to converge.\n",
        "\n",
        "Inside fit() function, you will need to implement the actual kmeans functionality. <br>\n",
        "The K-means process you should follow is listed below:\n",
        "1. Initialize each of the k centroids to a random datapoint if initialisation is not provided.\n",
        "2. Update each datapoint's cluster to that whose *centroid* is closest\n",
        "3. Calculate the new *centroid* of each cluster\n",
        "4. Repeat the previous two steps until no centroid value changes. Make sure you break out of the loop reagrdless of whether you converged or not once max iterations are reached.\n",
        "\n",
        "To help streamline this process, three helper functions have been given to you in the KMeans class \\\n",
        "- compute_distance(): use for step-2 above\n",
        "- find_closest_cluster(): use for step-2 above\n",
        "- compute_centroid(): use for step-3 above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ-_rjVLdncK"
      },
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "    '''Implementing Kmeans clustering'''\n",
        "\n",
        "    def __init__(self, n_clusters, max_iter=1000):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def compute_centroids(self, X, clusters):\n",
        "        \"\"\"\n",
        "        Computes new centroids positions given the clusters\n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        clusters -  m dimensional vector, where m is the number of training points \n",
        "                    At an index i, it contains the cluster id that the i-th datapoint \n",
        "                    in X belongs to.\n",
        "        \n",
        "        OUTPUT:\n",
        "        centroids - k by n matrix, where k is the number of clusters.\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
        "        # TODO your code here\n",
        "        ...\n",
        "\n",
        "        ## TODO end ##\n",
        "        return centroids\n",
        "\n",
        "    def compute_distance(self, X, centroids):\n",
        "        \"\"\"\n",
        "        Computes the distance of each datapoint in X from the centroids of all the clusters\n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        centroids - k by n matrix, where k is the number of clusters\n",
        "        \n",
        "        OUTPUT:\n",
        "        dist - m by k matrix, for each datapoint in X, the distances from all the k cluster centroids.\n",
        "        \n",
        "        \"\"\"\n",
        "        dist = np.zeros((X.shape[0], self.n_clusters))\n",
        "        # TODO your code here\n",
        "        ...\n",
        "\n",
        "        ## TODO end ##\n",
        "        return dist\n",
        "\n",
        "    def find_closest_cluster(self, dist):\n",
        "        \"\"\"\n",
        "        Finds the cluster id that each datapoint in X belongs to\n",
        "        \n",
        "        INPUT:\n",
        "        dist - m by k matrix, for each datapoint in X, the distances from all the k cluster centroids.\n",
        "        \n",
        "        OUTPUT:\n",
        "        clusters - m dimensional vector, where m is the number of training points \n",
        "                    At an index i, it contains the cluster id that the i-th datapoint \n",
        "                    in X belongs to.\n",
        "        \n",
        "        \"\"\"\n",
        "        clusters = np.zeros(dist.shape[0])\n",
        "        # TODO your code here\n",
        "        ...\n",
        "        ## TODO end ##\n",
        "        return clusters\n",
        "    \n",
        "    def fit(self, X, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Fit KMeans clustering to given dataset X. \n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        init_centroids (optional) - k by n matrix, where k is the number of clusters\n",
        "        \n",
        "        OUTPUT:\n",
        "        clusters - m dimensional vector, where m is the number of training points \n",
        "                    At an index i, it contains the cluster id that the i-th datapoint \n",
        "                    in X belongs to.\n",
        "        centroids - k by n matrix, where k is the number of clusters. \n",
        "                    These are the k cluster centroids, for cluster ids 0 to k-1\n",
        "        iters_taken - total iterations taken to converge. Should not be more than max_iter.\n",
        "        \n",
        "        \"\"\"\n",
        "        # Fix random seed. Do not change this!\n",
        "        np.random.RandomState(111)\n",
        "\n",
        "        ## TODO your code here ##\n",
        "        # Initialise centroids to random points in the dataset if not provided (i.e. None)\n",
        "        ...\n",
        "\n",
        "        # Iterate until kmeans converges or max_iters is reached. In each iteration: \n",
        "        #  - Update each datapoint's cluster to that whose *centroid* is closest\n",
        "        #  - Calculate the new *centroid* of each cluster\n",
        "        #  - Repeat the previous two steps until no centroid value changes. \n",
        "\n",
        "        ...\n",
        "        ## TODO end ##      \n",
        "        return self.clusters, self.centroids, iters_taken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuvtJjyudpVh"
      },
      "outputs": [],
      "source": [
        "# test case centroids should be aroudn (1.5,1.5) and (4.5,4.5)\n",
        "if NOTEBOOK:\n",
        "  points = []\n",
        "  result = []\n",
        "  random.seed(0)\n",
        "  for _ in range(500):\n",
        "    x = random.random()*3\n",
        "    y = random.random()*3\n",
        "    points.append((x,y))\n",
        "    result.append(0)\n",
        "  for _ in range(500):\n",
        "    x = random.random()*3 + 3\n",
        "    y = random.random()*3 + 3\n",
        "    points.append((x,y))\n",
        "    result.append(1)\n",
        "  clf = KMeans(2)\n",
        "  points = np.asarray(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZduFJWmmdp2y"
      },
      "outputs": [],
      "source": [
        "#test for sanity check\n",
        "def test_compute_centroids():\n",
        "  clf = KMeans(2)\n",
        "  centroid_p = clf.compute_centroids(np.array(points),np.array(result))\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        " [4.51568108,4.54138552]]\n",
        "  assert(np.linalg.norm(centroid_p - np.array(centroid_r)) <= 1e-2 )\n",
        "if NOTEBOOK:\n",
        "  test_compute_centroids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvP10Sgjdp9i"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_compute_centroids', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxu-WPKXdqyR"
      },
      "outputs": [],
      "source": [
        "def test_distance():\n",
        "    centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "    clf = KMeans(2)\n",
        "    distance = clf.compute_distance(np.array(points),np.array(centroid_r))\n",
        "    distance_for_0 = [1.30098366, 3.01191447]\n",
        "    assert(np.linalg.norm(distance_for_0-distance[0]) <= 1e-2)\n",
        "if NOTEBOOK:\n",
        "  test_distance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQEQdSf7dq0V"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_distance', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_eYli_idq2G"
      },
      "outputs": [],
      "source": [
        "def test_find_clusters():\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "  clf = KMeans(2)\n",
        "  distance = clf.compute_distance(np.array(points),np.array(centroid_r))\n",
        "  cluster = clf.find_closest_cluster(distance)\n",
        "  assert(cluster[0] == 0)\n",
        "if NOTEBOOK:\n",
        "  test_find_clusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib9uRPtJdq3q"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_find_clusters', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFmAR2IJdx2j"
      },
      "outputs": [],
      "source": [
        "def test_fit():\n",
        "  clf = KMeans(2)\n",
        "  clusters, centroids, _ = clf.fit(np.array(points),np.array([[1,1],[4,4]]))\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "  assert(np.linalg.norm(centroids - np.array(centroid_r)) <= 1e-2 )\n",
        "  assert(sum(np.array(clusters)-np.array(result)) == 0)\n",
        "if NOTEBOOK:\n",
        "  test_fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iv6H8xmdx5H"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_fit', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjBNHRAd5Sy"
      },
      "source": [
        "## **3.3. [3 pts] Compute distortion**\n",
        "\n",
        "As you may have noticed already, one big question still remains. How do we know what value of k to choose?\n",
        "\n",
        "One way to decide on a value for k is to run k-means and plot the distortion (sum of squared error based on the Euclidean distance). From that we can find the \"elbow of the graph\" that indicates the best tradeoff between number of clusters and corresponding distortion.\n",
        "\n",
        "In the function `test_cluster_size`,  iterate over possible cluster sizes from 2 to a `max_cluster` (inclusive) value. For each *k*, run k-means and calculate its distortion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QadOknBbd5Yl"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    def test_cluster_size(X, max_k):\n",
        "        \"\"\"\n",
        "        Iterates over possible cluster from 2 to max_k, running k-means and calulating distortion.\n",
        "        \n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        max_k - the maximum number of clusters to consider\n",
        "        \n",
        "        OUTPUT:\n",
        "        scores - a list of scores, that contains the distortion for k = 2 to max_k, in order.\n",
        "        \"\"\"\n",
        "        scores = [0] * (max_k-1)\n",
        "        ## TODO your code here ##\n",
        "        ...\n",
        "        \n",
        "        ## TODO end ##\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8efXTjqHd8iP"
      },
      "outputs": [],
      "source": [
        "def test_test_cluster_size():\n",
        "  scores = test_cluster_size(np.array(points),5)\n",
        "  assert(np.argmax(scores) == 0)\n",
        "if NOTEBOOK:\n",
        "  test_test_cluster_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBiO6N-jd89g"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    max_k = 20\n",
        "    scores = test_cluster_size(X, max_k)\n",
        "    grader.grade(test_case_id = 'test_test_cluster_size', answer = scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxg6MtUXd_zG"
      },
      "source": [
        "## **3.4. [3 pts, manually graded] Plot distortion vs. k (without feature scaling)** \n",
        "\n",
        "Plot distortion vs. different k values by using the function we just wrote on dataset X and add it in the written report. Use max_k = 20. Determine the best k value from this plot and also mention it in the written report. Make sure your plot has axes labels, legend and title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou2XslKPd9AK"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    ## TODO your code here ##\n",
        "    ...\n",
        "    ## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdyEHPbeDgb"
      },
      "source": [
        "## **3.5. [3 pts, manually graded] Plot distortion vs. k (with feature scaling)** \n",
        "\n",
        "What we just did was running k-means clustering over the dataset X without any feature scaling. This time, we will rescale each feature to the standard range of (0,1) before passing it to k-means and computing the distortion.\n",
        "\n",
        "Use `sklearn.preprocessing.MinMaxScaler` ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)) and scale the dataset X before passing it to the `test_cluster_size` function. As before, plot distortion vs. different k values and add it in the written report. Use max_k = 20. Determine the best k value from this plot and also mention it in the written report. Make sure your plot has axes labels, legend and title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeqKP3g7eEGN"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    ## TODO your code here ##\n",
        "    # Use min-max scaler to scale the dataset\n",
        "    ...\n",
        "\n",
        "    ## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JCX-tmzeHLV"
      },
      "source": [
        "## **3.6. [4 pts, manually graded] Comments**\n",
        "\n",
        "Answer these questions in the written report.\n",
        "\n",
        "1. Why do you get different results with and without feature scaling?\n",
        "2. Should you scale the features before fitting k-means? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Nw2VyudyPs"
      },
      "source": [
        "# **4. [18 pts, 5 autograded, 13 manually graded] Principal Component Analysis** \n",
        "\n",
        "## **4.1. [8 pts, manually graded] Exploring Effects of Different Princple Components in Linear Regression**\n",
        "We have introduced you a way of dimension reduction, Principal Component Analysis, in class. Now, we would like to ask you to apply PCA from sklearn on the breast cancer dataset to observe its performance and interpret the major components.\n",
        "\n",
        "In order to better compare the effects of PCA, we load the labels from the dataset.\n",
        "\n",
        "Then, we will evaluate the performances of raw dataset and various numbers of pca components on LinearRegression classifier.\n",
        "\n",
        "In the section, you are asked to draw a plot of test accuracies vs number of different principle components. The detailed instructions are included in the following cells.\n",
        "\n",
        "Remember to **attach the plot** in your written submission, and also **make comments** about what you observe, explain the reason behind the trend, and what conclusion you could draw from the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhGf2QNbfI4o"
      },
      "outputs": [],
      "source": [
        "# load the label from the dataset, which is a binary label 0/1 representing whether the cancer is benign or malignant\n",
        "\n",
        "## TODO your code here ##\n",
        "...\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7HJgqEDfcOT"
      },
      "outputs": [],
      "source": [
        "# try raw data vs PCA data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "## TODO your code here ##\n",
        "# Step 1: split the data into train and test set by a test_size of 0.33.\n",
        "...\n",
        "\n",
        "# Step 2: Train a linear regression model using train set and predict on the test set.\n",
        "# As the labels are binary, we should cast the predictions into binary labels as well. (Set predictions >=0.5 as 1)\n",
        "\n",
        "# You might want to print out accuracy scores here\n",
        "print(\"Raw data has accuracy: \", raw_accu)\n",
        "\n",
        "\n",
        "# Step 3: Iterate the number of components from 1 to 10 (exclusive). \n",
        "# For each number of PCs, we are training a linear regression model and save its accuracy on the test set following the same style as above.\n",
        "# Remeber to only fit the train set and not the test set. \n",
        "# You might want to store your accuracies in a list\n",
        "accuracies = []\n",
        "\n",
        "...\n",
        "\n",
        "# Step 4: Make a plot to compare accuracy vs number of PCs on Linear Regression for the test set.\n",
        "# Add a black, dashed line for the test accuracy of linear regression by feeding the raw input data.\n",
        "# Remeber to add x, y labels and title to your plot, and comment on your observations.\n",
        "...\n",
        "\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwlnQRmqoKoU"
      },
      "source": [
        "## **4.2. [5 pts] Understanding PCA**\n",
        "\n",
        "### **4.2.1 [2 pts, autograded] Explained Ratio of PCA**\n",
        "Given a threshold of explained ratio (0 < ratio < 1), compute the number of required PCs to reach the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWE8VTsguO7Y"
      },
      "outputs": [],
      "source": [
        "def select_n_principal_components(data, variation):\n",
        "  ## TODO your code here ##\n",
        "  ...\n",
        "\n",
        "  ## TODO end ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrkdjex2qMb0"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "  student_ans = [select_n_principal_components(cancer_dataset['data'], 0.98), select_n_principal_components(cancer_dataset['data'], 0.99)]\n",
        "  grader.grade(test_case_id = 'test_select_n_principal_components', answer = student_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF0A9640q_cd"
      },
      "source": [
        "### **4.2.2 [3 pts, autograded] Composition of PCA**\n",
        "In this section, we ask you to understand which features specifically in the dataset contribute to the important PCs. We ask that you select the best number of principle components you got from previous part and analyze its composition (as there are multiple components contributing to each PC, you only need to specify the **top three** features that are explained by these PCs together)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkjwg6H6u23u"
      },
      "outputs": [],
      "source": [
        "## TODO your code here ##\n",
        "...\n",
        "\n",
        "## TODO end ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpNOIfoWnAQq"
      },
      "source": [
        "## **4.3. [5 pts, manually graded] PCA and KMeans**\n",
        "We first run PCA on the dataset for visualisation in 2D space. Note that k-means is actually being fit on the entire feature set. \n",
        "\n",
        "Next, call your k-means class on the dataset X and obtain the clusters. Make sure to populate the \"clusters\" variable here. We have provided the plotting code for you.\n",
        "\n",
        "**Add these plots in the written report.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWI-2KQmd29p"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # PCA for visualisation in 2D. \n",
        "    pca = PCA(n_components = 2)\n",
        "    v = pca.fit(np.transpose(X)).components_\n",
        "\n",
        "    for k in [3,5,7,9, 11]:\n",
        "\n",
        "        clusters = np.zeros(X.shape[0])\n",
        "\n",
        "        ## TODO your code here ##\n",
        "        ...\n",
        "\n",
        "        ## TODO end ##\n",
        "\n",
        "        plt.scatter(v[0], v[1], c=clusters, s=18)\n",
        "        plt.title(\"Breast Cancer Clusters (k = \"+str(k) + \")\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD6VKdQIwcxg"
      },
      "source": [
        "## Submit to Gradescope\n",
        "Congratulations! You've finished the homework. Don't forget to submit your final notebook on [Gradescope](gradescope.com)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "377c3e77380f886ab555d62b93e59a1648fc55affccd8d0220be3281f77f4c6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}