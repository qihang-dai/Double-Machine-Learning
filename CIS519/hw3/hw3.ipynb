{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnokyRPqBJ7n"
      },
      "source": [
        "# **CIS 4190/5190 Homework 3 - Spring 2023**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCMO-KSHept"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpaeiBrHz7Xy",
        "outputId": "b01cf2da-6fb3-4047-aa87-aecc6d14eb3d"
      },
      "outputs": [],
      "source": [
        "# For autogreader only, do not modify this cell. \n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJsS5fTThkLU"
      },
      "source": [
        "## **PennGrader Setup**\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**. \n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF4jSeEqhkLV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install penngrader --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DolsQ5fDhkLV"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppXdHvskhkLV"
      },
      "source": [
        "## **Autograder Setup**\n",
        "Enter your 8-digit PennID below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7nuuZCohkLW"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 103          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuZcxcXJhkLW"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immediately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omOwgdzohkLW",
        "outputId": "1465139c-83e8-4e17-f829-85fac243a2bf"
      },
      "outputs": [],
      "source": [
        "grader = PennGrader(homework_id = 'CIS5190_Sp23_HW3', student_id = STUDENT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCxRFpFfp8fl"
      },
      "outputs": [],
      "source": [
        "# Serialization code needed by the autograder\n",
        "import inspect, sys\n",
        "from IPython.core.magics.code import extract_symbols\n",
        "\n",
        "def new_getfile(object, _old_getfile=inspect.getfile):\n",
        "    if not inspect.isclass(object):\n",
        "        return _old_getfile(object)\n",
        "    \n",
        "    # Lookup by parent module (as in current inspect)\n",
        "    if hasattr(object, '__module__'):\n",
        "        object_ = sys.modules.get(object.__module__)\n",
        "        if hasattr(object_, '__file__'):\n",
        "            return object_.__file__\n",
        "    \n",
        "    # If parent module is __main__, lookup by methods (NEW)\n",
        "    for name, member in inspect.getmembers(object):\n",
        "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
        "            return inspect.getfile(member)\n",
        "    else:\n",
        "        raise TypeError('Source for {!r} not found'.format(object))\n",
        "inspect.getfile = new_getfile\n",
        "\n",
        "def grader_serialize(obj):\n",
        "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
        "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
        "    return class_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7O8ygZvL-H"
      },
      "source": [
        "## **Datasets**\n",
        "Next, we will download all datasets from Google Drive to your local runtime. After successful download, you may verify that all datasets are present in your colab instance.\n",
        "\n",
        "HW1 Datasets:\n",
        "- [cis519_hw1_diabetes_train.csv](https://drive.google.com/file/d/1wfZ375m-HOvtWb8fBUjZ2zbW5nkXtlVx/view?usp=sharing)\n",
        "- [cis519_hw1_diabetes_test.csv](https://drive.google.com/file/d/14DGsr_eIHRGnDw4o7FVBFUVfg_MljzzO/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5THxiIJBvH_F",
        "outputId": "6e378a69-7d47-4743-b6ca-adcb0163bce6"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    if not os.path.exists(\"cis519_hw1_diabetes_train.csv\"):\n",
        "        !gdown --id 1wfZ375m-HOvtWb8fBUjZ2zbW5nkXtlVx\n",
        "    if not os.path.exists(\"cis519_hw1_diabetes_X_test.csv\"):\n",
        "        !gdown --id 14DGsr_eIHRGnDw4o7FVBFUVfg_MljzzO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RUGn-V6Nnjt"
      },
      "source": [
        "#### **NOTE: Results of sections marked as \"manually graded\" should be submitted along with the written homework solutions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-m0iExC9bm4"
      },
      "source": [
        "# **1. Logistic Regression** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9uBsNwS9c32"
      },
      "source": [
        "## **1.1. Logistic Regression Implementation [18 pts, autograded]**\n",
        "\n",
        "Implement logistic regression with both L1 and L2 regularization by completing the LogisticRegression class.  \n",
        "\n",
        "Your class must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `sigmoid(x)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `fit(X, y)`\n",
        "* `predict_proba(X)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LogisticRegression class. **DO NOT** change the API.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.1. Sigmoid Function [1 pt]**\n",
        "\n",
        "You should begin by implementing the `sigmoid` function.  As you may know, the sigmoid function $\\sigma(x)$ is mathematically defined as follows.\n",
        "\n",
        "> $\\sigma(x) = \\frac{1}{1\\ +\\ \\text{exp}(-x)}$\n",
        "\n",
        "**Be certain that your sigmoid function works with both vectors and matrices** --- for either a vector or a matrix, you function should perform the sigmoid function on every element.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.2. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
        "$\n",
        "\n",
        "where\n",
        "> $\n",
        "h_{\\theta}(x_{i}) = \\sigma(\\theta^{T}x_{i})\n",
        "$\n",
        "\n",
        "\n",
        "L1 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.3. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.4. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 2.1.5 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **1.1.5. Training [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights start as a zero vector. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.6. Predict Probability [1 pt]**\n",
        "\n",
        "The `predict_probability` function should predict the probabilities that the data points in a given input data matrix belong to class 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.7. Predict [2 pts]**\n",
        "\n",
        "The `predict` function should predict the classes of the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVWOemIR8-09"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.01, tol=0.0001, max_iter=10000, theta_init=None, penalty = None, lambd = 1.0):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.theta_init = theta_init\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # a function needed for using cross_val_score function from sklearn.model_selection\n",
        "        return {\"alpha\": self.alpha, \"max_iter\": self.max_iter, \"lambd\" : self.lambd, \"penalty\" : self.penalty}\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the sigmoid value of the argument.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: numpy.ndarray\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: numpy.ndarray\n",
        "            The sigmoid value of x\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "\n",
        "    def compute_cost(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        # DO NOT use np.dot for this function as it can possibly return nan. Use a combination of np.nansum and np.multiply.\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "        \n",
        "    def compute_gradient(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the probabilities that the data points in X belong to class 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted probabilities that the data points in X belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4edZSvLt8_Km"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_sigmoid(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case = np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n",
        "    student_ans = student_lr_clf.sigmoid(test_case)\n",
        "    required_ans = np.array([0.83539354, 0.35165864, 0.3709434 , 0.25483894, 0.70378922])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_sigmoid(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syK4y-vy8_Nd",
        "outputId": "a9833c03-dce4-4856-d4c6-7a282c67375a"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_sigmoid', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p37bE3an8_QK"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_compute_cost(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.467975765663204\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.52915138076548\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.505400330283089\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_compute_cost(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok-FpSND8_Sb",
        "outputId": "d6445616-7b2e-4369-cb97-17f3fd234a59"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_compute_cost', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI-neAfu8_Ub"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_compute_gradient(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.20203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.30203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.32438267])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_compute_gradient(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5rhezuv8_WC",
        "outputId": "c016a9e7-cc20-491c-d3e0-e448d865404d"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_compute_gradient', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhQ0YqCU-SeO"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_has_converged(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_clf.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "\n",
        "    assert student_ans == required_ans\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_has_converged(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcUdoY2q-Sg-",
        "outputId": "6f77265a-d1bc-421f-d7ad-38b5f09d7756"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_has_converged', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJC7W7Lh-SjY"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_fit(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "                             [ 0.005     , -0.00597503,  0.00564325],\n",
        "                             [ 0.01006813, -0.01184464,  0.0111865 ],\n",
        "                             [ 0.01520121, -0.01761226,  0.01663348],\n",
        "                             [ 0.02039621, -0.02328121,  0.02198778],\n",
        "                             [ 0.02565018, -0.0288547 ,  0.02725288]])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_fit(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKtP_9lP-Sla",
        "outputId": "c8a8b3ba-69eb-46dc-fb9b-f39b35c2bddf"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_fit', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toqYtIpX-Snf"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_predict_proba(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict_proba(test_case_X)\n",
        "    required_ans = np.array([0.49052814, 0.5029122 , 0.48449386, 0.48864172, 0.50241207])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_predict_proba(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EOHKNxs-ajX",
        "outputId": "b6e9ca57-bc1a-415f-fcc2-0eb16d5fd19a"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_predict_proba', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6yb0tUR-alL"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_predict(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict(test_case_X)\n",
        "    required_ans = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
        "                             0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
        "\n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 0.02\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_predict(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfP3FHnn-dzK",
        "outputId": "8b384d45-0289-40e6-c8c8-065ffcd8b892"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_predict', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R1hHz4C-hWy"
      },
      "source": [
        "## **1.2. Effect of learning rate on gradient descent [5 pts, manually graded]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGik8OTr-rKn"
      },
      "source": [
        "Run the below cell to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekqfVecV-d1b"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    \n",
        "    if not os.path.exists(\"cis519_hw2_admit.csv\"):\n",
        "        !gdown --id 1CSD1vA9qZucuevxCuaOwr91tBaZcjNNh\n",
        "    \n",
        "    train_df = pd.read_csv(\"cis519_hw2_admit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnrAuubQ-v10"
      },
      "source": [
        "The dataset contains two features - scores in two exams and the target variable is whether the student was admitted into a college or not. Your task for this question is to use this dataset and plot the variation of cost function with respect to the number of gradient descent iterations for different learning rates. Perform the following steps.\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates - {0.001, 0.01, 0.1, 0.25}, fit a logistic regression model to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n",
        "\n",
        "Submit the plot along with the written homework solutions. The plot should have an appropriate title, axes labels, and legend. Briefly comment on the effect of increasing learning rate and what would be the best learning rate among the four values based on the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "tpRpmIHd-d3D",
        "outputId": "d79decb7-6789-4aa9-8cf1-cb28aa7e230a"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # STUDENT CODE STARTS:\n",
        "    ...\n",
        "    \n",
        "    # STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWRRLni5Pz_Q"
      },
      "source": [
        "# **2. K-Nearest Neighbors [10 pts]**\n",
        "While doing classification, KNN searches the memorized training instances for the K instances that most closely resemble the new instance and assigns to it the most common class. An alternate way of understanding KNN is by looking at the learned decision boundaries. In this problem, you will implement a function to classify points in the X-Y coordinates using [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). The training dataset used is the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris), and each point in the 2d-space will be classified into one of the three classes using its x-coordinate(sepal length) and y-coordinate(sepal width)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mze8zKRY4c3z"
      },
      "source": [
        "## **2.1. Load Iris Dataset**\n",
        "Please complete the load_dataset function to\n",
        "- Populate X_train with iris dataset features. We use only the sepal length and width for this exercise, i.e. the first two columns in the dataset\n",
        "- Populate y_train with labels (species)\n",
        "- return X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay4oEx1x4eeu"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "def load_iris_dataset():\n",
        "    '''\n",
        "    Args:\n",
        "        None\n",
        "    Returns: X_train, y_train\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "    '''\n",
        "    # import training data\n",
        "    iris = datasets.load_iris()\n",
        "\n",
        "    # TODO:\n",
        "    # Examine the iris variable and initialize the following variables appropriately:\n",
        "    # 1. X_train - Shape (m, 2): Only use the sepal length and width\n",
        "    # 2. y_train - Shape (m, ): target labels \n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "# Load the iris dataset first\n",
        "X_train, y_train = load_iris_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYAL32DU2cWJ"
      },
      "source": [
        "## **2.2. Standardise the Features [4 pts, autograded]**\n",
        "Please complete the standardise_features function to \n",
        "standardize the features by subtracting the mean and scaling to unit variance. i.e \n",
        "\n",
        " z = (x - u) / s\n",
        "\n",
        "where u is the mean of the training data and s is the standard deviation of the training data.\n",
        "\n",
        "Here, centering and scaling need to happen independently on each feature (column) of the training data.\n",
        "\n",
        "**Note**: \n",
        "\n",
        "Please implement this function yourself. \n",
        "\n",
        "**Do NOT use sklearn's StandardScaler**. \n",
        "\n",
        "You are encouraged to use numpy as well as numpy vectorisation/broadcasting techniques to speed up the calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn2eDuOj170T"
      },
      "outputs": [],
      "source": [
        "def standardise_features(X_train):\n",
        "\n",
        "  '''\n",
        "  Args:\n",
        "      X_train: Training dataset\n",
        "  Returns: X_train (After Standardization)\n",
        "  Notes:\n",
        "      1. Please do not change the provided code\n",
        "  '''\n",
        "\n",
        "  # TODO:\n",
        "  # 1. Calculate columnwise means and standard deviations\n",
        "  # 2. Perform columnwise standardisation i.e. subtract off mean and divide by standard deviation\n",
        "  # 3. Return the standardised data\n",
        "  #### Student code starts ####\n",
        "  ...\n",
        "  \n",
        "  #### Student code ends ####\n",
        "\n",
        "X_train = standardise_features(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z2ZFHXN2EVq",
        "outputId": "90d7d52a-b034-4085-a746-8789920f7570"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_knn_standardise', answer = grader_serialize(standardise_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHobVEPHxXa8"
      },
      "source": [
        "## **2.3. Plot KNN Decision Boundary [6 pts, manually graded]**\n",
        "Please complete the plot_KNN_boundary function to\n",
        "- train a KNN classifier with k neighbors using the provided X_train and y_train\n",
        "- make predictions using X_test and save the result as 'y_test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQNVfjcz4tj5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def plot_KNN_boundary(k, X_train, y_train):\n",
        "    '''\n",
        "    Args:\n",
        "        k: Number of neighbors to use for kneighbors queries.\n",
        "        X_train: Training dataset\n",
        "        y_train: Labels\n",
        "\n",
        "    Returns:\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "        2. save the predicted labels as y_test for plotting\n",
        "    '''\n",
        "\n",
        "    # Mesh 2d space into grid to generate X_test and y_test\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                            np.arange(y_min, y_max, h))\n",
        "    X_test = np.c_[xx.ravel(), yy.ravel()]\n",
        "    y_test = np.zeros(xx.shape)\n",
        "\n",
        "    # TODO:\n",
        "    # 1. train a KNN classifier\n",
        "    # 2. save the predictions on X_test in y_test\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    y_test = y_test.reshape(xx.shape)\n",
        "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
        "    cmap_bold = ['darkorange', 'c', 'darkblue']\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, y_test, cmap=cmap_light)\n",
        "\n",
        "    # Also plot the training points\n",
        "    iris_target_names = ['setosa', 'versicolor', 'virginica']\n",
        "    sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=map(lambda y: iris_target_names[y], y_train),\n",
        "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.title(\"3-Class classification (k = %i)\" % (k))\n",
        "    plt.xlabel(\"standardised sepal length\")\n",
        "    plt.ylabel(\"standardised sepal width\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvb0XeDyAEC"
      },
      "source": [
        "Explore the effect of changing k on the learned decision boundaries. \n",
        "- Submit the plots along with the written homework solutions. \n",
        "- State whether the model underfits/overfits as k increases and explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wbUJFxTcqTgP",
        "outputId": "12e981a7-d023-4159-cee1-43e5cbfd49a1"
      },
      "outputs": [],
      "source": [
        "# Plot KNN decision boundaries with different k values\n",
        "def visualize_KNN(X_train, y_train):\n",
        "\n",
        "    k_list = [1, 4, 11, 15]\n",
        "    \n",
        "    # TODO:\n",
        "    # 1. Call plot_KNN_boundary function for each value of k in k_list\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "    visualize_KNN(X_train, y_train) ### Comment out this line when submitting ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Cm728599Wt"
      },
      "source": [
        "# **3. Measures of Impurity and their Reduction [15 pts]**\n",
        "To grow a classification tree, instead of a binary error (1/0), measures of impurity are used to see how good a leaf node is. Recall that we discussed about entropy being one such measure of impurity. We will be working with entropy and comparing it to another metric called the gini index. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FHSKW6f_7yg"
      },
      "source": [
        "## **3.1. Measures of Impurity [9 pts]**\n",
        "\n",
        "For this problem, consider that you have a binary classification problem of two classes, the positive class $1$ and the negative class $0$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aELrA7A_WbJ8"
      },
      "source": [
        "### **3.1.1. Entropy [2 pts, autograded]**\n",
        "\n",
        "Please complete the entropy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dyEposN3Sph"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cross_entropy(prob_class1):  \n",
        "\n",
        "    \"\"\"\n",
        "    Returns the cross-entropy value of a node given the probability of a sample belonging to class 1 in the node.\n",
        "\n",
        "    Args: \n",
        "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
        "      \n",
        "    Returns:\n",
        "        ce: The cross-entropy value for the node\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends #### \n",
        "    \n",
        "assert cross_entropy(0.5) == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE5spiSJp8e7",
        "outputId": "c108aed0-49c4-4ff4-9739-0d064cb1d9a8"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_cross_entropy', answer = grader_serialize(cross_entropy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxp71lMkdoVP"
      },
      "source": [
        "### **3.1.2. Gini Index [2 pts, autograded]**\n",
        "\n",
        "Gini index is another measure of impurity. For an K-class classification problem, gini index is calculated as follows.\n",
        "\n",
        "$$\\text{Gini Index} = \\sum_{k = 1}^{K} p_k(1 - p_k)$$\n",
        "\n",
        "Complete the following function for calculating the gini index of a binary-class problem (k = 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj__P_XV3Uj5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gini_index(prob_class1):  \n",
        "\n",
        "    \"\"\"\n",
        "    Returns the gini-index value of a node given the probability of a sample belonging to class 1 in the node.\n",
        "\n",
        "    Args: \n",
        "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
        "      \n",
        "    Returns:\n",
        "        gi: The gini-index value for the node\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends #### \n",
        "\n",
        "assert gini_index(0.5) == 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMOROQZH09Im",
        "outputId": "465095dc-5075-49da-bb3e-4a70ef849471"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_gini_index', answer = grader_serialize(gini_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "837Ex93tXP1c"
      },
      "source": [
        "### **3.1.2. Plot [5 pts, manually graded]**\n",
        "\n",
        "Please complete the impurity_measures_plot function and generate a plot of the entropy and gini index values with respect to the class 1 probability values. Both the impurity measures should be on the same plot.\n",
        "\n",
        "- Submit the generated plot along with the written homework solutions.\n",
        "- Make sure the plot has a title, legend and axes labels.\n",
        "- Comment on why cross entropy and gini index are suitable measures of impurity based on the plot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "J1lZ_hdi-Bwc",
        "outputId": "2f575e4b-969a-4d1d-f749-412f17029646"
      },
      "outputs": [],
      "source": [
        "def impurity_measures_plot():\n",
        "\n",
        "    '''\n",
        "    Plots the cross entropy and gini index values with respect to the probability values of class 1.\n",
        "\n",
        "    Args: \n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Notes:\n",
        "        1. Please do not change the provided code\n",
        "        2. Both cross entropy and gini index should be on the same scatter plot\n",
        "    '''\n",
        "\n",
        "    prob_class1_arr = np.arange(1, 1000)/1000\n",
        "    ce_arr = np.array([cross_entropy(p) for p in prob_class1_arr])\n",
        "    gi_arr = np.array([gini_index(p) for p in prob_class1_arr])\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "  impurity_measures_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow8qK0lZ2_JM"
      },
      "source": [
        "## **3.2. Reduction in Impurity [6 pts]**\n",
        "\n",
        "Recall that we also discussed information gain which is the change in entropy from the parent node to the children nodes. Gini reduction is similar to information gain except you replace entropy values with gini index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMeFW6DgPCFe"
      },
      "source": [
        "### **3.2.1. Information Gain [3 pts, autograded]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZvwxSjc3dHT"
      },
      "outputs": [],
      "source": [
        "def information_gain(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Args: \n",
        "        num_samples_parent: Number of samples in the parent node\n",
        "        num_class1_parent: Number of samples of class 1 in parent node\n",
        "        num_samples_child1: Number of samples in the first child node\n",
        "        num_class1_child1: Number of samples of class 1 in the first child node\n",
        "\n",
        "    Returns:\n",
        "        ig: Information Gain\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    # 1. You will need to calculate cross-entropy for the parent and child nodes\n",
        "    # 2. Use the above entropies to finally calculate information gain\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "\n",
        "assert np.abs(information_gain(100, 60, 30, 5) - 0.251) < 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GELCUsbFG18O",
        "outputId": "0bc57ceb-66a2-4557-a671-b85d77baef80"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_information_gain', answer = (grader_serialize(cross_entropy), grader_serialize(information_gain)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUxtCnOPTBQ"
      },
      "source": [
        "### **3.2.2. Gini Reduction [3 pts, autograded]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCiXGB--6MR5"
      },
      "outputs": [],
      "source": [
        "def gini_reduction(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Args: \n",
        "        num_samples_parent: Number of samples in the parent node\n",
        "        num_class1_parent: Number of samples of class 1 in parent node\n",
        "        num_samples_child1: Number of samples in the first child node\n",
        "        num_class1_child1: Number of samples of class 1 in the first child node\n",
        "\n",
        "    Returns:\n",
        "        gr: Gini Reduction\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "assert np.abs(gini_reduction(100, 60, 30, 5) - 0.161) < 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldMLkWpS-cLv",
        "outputId": "f829282c-a109-4595-eb0a-52ddf2411c8a"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_gini_reduction', answer = (grader_serialize(gini_index), grader_serialize(gini_reduction)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JG4XjHNPuC2"
      },
      "source": [
        "# **4. Decision Tree [35 pts]**\n",
        "\n",
        "In this section you will be training a decision tree classifier to predict the presence of diabetes in a person given various input features. The diabetes dataset that we are using is from the [2013-2014  National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/Default.aspx). We have reduced the dataset to only 20 features but the original dataset had over 1,800 features. `cis519_hw1_diabetes_train.csv` and `cis519_hw1_diabetes_X_test.csv` are the datasets that you would be using for training and testing respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC_OXMz-oRKD"
      },
      "source": [
        "## **4.1. Load Datasets**\n",
        "\n",
        "Read the files `cis519_hw1_diabetes_train.csv` and `cis519_hw1_diabetes_X_test.csv` into train_df and test_df respectively in the `load_diabetes_datasets` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ4pBCjm_5-K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_diabetes_datasets():\n",
        "    '''\n",
        "    Args:\n",
        "        None\n",
        "    Returns: \n",
        "        train_df, test_df\n",
        "    '''\n",
        "    \n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06na9yloolX"
      },
      "source": [
        "## **4.2. Preprocess Datasets [10 pts, autograded]**\n",
        "\n",
        "The datasets we have provided are not ready-to-use for machine learning and requires preprocessing. We want you to perform feature selection and handle missing values in both the training and test datasets. \n",
        "\n",
        "### **4.2.1. Feature Selection**\n",
        "\n",
        "For feature selection, you should retain the following features at least and experiment including/excluding the remaining features. \n",
        "\n",
        "- 'RIDAGEYR'\n",
        "- 'BMXWAIST'\n",
        "- 'BMXHT'\n",
        "- 'LBXTC'\n",
        "- 'BMXLEG'\n",
        "- 'BMXWT'\n",
        "- 'BMXBMI'\n",
        "- 'RIDRETH1'\n",
        "- 'BPQ020'\n",
        "- 'ALQ120Q'\n",
        "- 'DMDEDUC2'\n",
        "- 'RIAGENDR'\n",
        "- 'INDFMPIR'\n",
        "\n",
        "The column `DIABETIC` in the training dataset is the target variable. \n",
        "\n",
        "### **4.2.2. Handling Missing Values**\n",
        "\n",
        "We recommend you to drop rows with missing values in the training set. However, you should not drop rows with missing values in the test set. Instead, you should impute missing values in the test set with the mean of the corresponding columns in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoEdsqDgBC49"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    train_df, test_df = load_diabetes_datasets()\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_datasets(train_df, test_df):\n",
        "    '''\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "    Returns:\n",
        "        train_df (preprocessed)\n",
        "        test_df (preprocessed)\n",
        "    Note:\n",
        "        1. At least the following columns should be present in the final train_df:\n",
        "            - 'RIDAGEYR'\n",
        "            - 'BMXWAIST'\n",
        "            - 'BMXHT'\n",
        "            - 'LBXTC'\n",
        "            - 'BMXLEG'\n",
        "            - 'BMXWT'\n",
        "            - 'BMXBMI'\n",
        "            - 'RIDRETH1'\n",
        "            - 'BPQ020'\n",
        "            - 'ALQ120Q'\n",
        "            - 'DMDEDUC2'\n",
        "            - 'RIAGENDR'\n",
        "            - 'INDFMPIR'\n",
        "            - 'DIABETIC'\n",
        "        2. test_df will have all the columns in train_df except the 'DIABETIC' column \n",
        "        3. Drop any rows in train_df that have missing values\n",
        "        4. DO NOT drop rows with missing values test_df. Impute missing values in test_df with the means of the corresponding columns in train_df. \n",
        "    '''\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkhFIXIFZdcQ",
        "outputId": "2b884155-fc14-4dfe-cd80-4fd19d85fc6d"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # PennGrader Grading Cell\n",
        "    grader.grade(test_case_id = 'test_preprocess_datasets', answer = preprocess_datasets(train_df, test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0k07SoBrrD6"
      },
      "source": [
        "## **4.3. Decision Tree Training with Pruning [14 pts autograded]**\n",
        "\n",
        "Next, we will be fitting a decision tree classifier and prune the tree appropriately. The `DecisionTreeClassifier` in scikit-learn uses a way of pruning called **Minimal Cost-Complexity Pruning**. We won't cover the specifics, but you can learn more from this [link](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning) if you wish. But, you don't need to learn the details in order to use it effectively. The amount of pruning is entirely dependent on the value of the `ccp_alpha` parameter. In order to tune the `ccp_alpha` parameter, you will use [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). The purpose of cross-validation is to estimate how well a model will generalize on unseen data.\n",
        "\n",
        "Implement the function `best_ccp_alpha_f1` to do automatic tuning of the `ccp_alpha` parameter.  Your function should vary the value of the `ccp_alpha` parameter and return the value for `ccp_alpha` with the highest cross-validation F1 score over the given dataset `train_df`. The sklearn library has a [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path) to generate a list of effective ccp_alphas. Given the imbalanced nature of the dataset, most of the people in the data set are non-diabetic. You can get a model with very high test accuracy by always predicting no one is diabetic. To address this problem, more importance should be given to the [F1 score](https://en.wikipedia.org/wiki/F-score) of your model rather than the classification accuracy.\n",
        "\n",
        "Once you are done with your modeling process, test your model on the test dataset and output your predictions into the PennGrader.\n",
        "\n",
        "For this problem, you need to have at least 80% accuracy and a F1 score of 0.2 on the test dataset to get full points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOlr7qzjxIxS"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "if NOTEBOOK:\n",
        "    train_df, test_df = preprocess_datasets(train_df, test_df)\n",
        "\n",
        "def best_ccp_alpha_f1(train_df):\n",
        "    \"\"\"\n",
        "    Returns the pruning parameter (best_ccp_alpha) with the highest cross-validation F1 score along with the \n",
        "    five cross-validation F1 scores corresponding (cv_f1_scores).\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "\n",
        "    Returns:\n",
        "        best_ccp_alpha: the tuned best ccp alpha value\n",
        "        cv_f1_scores: the five cross-validation F1 scores\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "\n",
        "    #### Student code ends ####    \n",
        "\n",
        "def refit_and_predict(train_df, test_df, best_ccp_alpha):\n",
        "    \"\"\"\n",
        "    Fit a decision tree classifier on the training data using the best_ccp_alpha value and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "        best_ccp_alpha\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "    best_ccp_alpha, cv_f1_scores = best_ccp_alpha_f1(train_df)\n",
        "    y_test_pred = refit_and_predict(train_df, test_df, best_ccp_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K2RNN3Tg6jg",
        "outputId": "fe569b38-69da-47de-f0df-c935d5a39dba"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # PennGrader Grading Cell\n",
        "    grader.grade(test_case_id = 'test_refit_and_predict', answer = y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khcu-0rBipeR"
      },
      "source": [
        "## **4.4. Computing Confidence Intervals [5 pts, autograded]**\n",
        "\n",
        "Even though you may have computed the average F1 score across the held-out folds during cross validation, how confident can you be that the number you computed is the true F1 score for that set of features? If you try rerunning your code with a different random seed, you may actually get a different F1 score. But which one is right?\n",
        "\n",
        "In order to answer this question, we will compute a confidence interval based on the Student's t-distribution, which will tell us with 99\\% confidence that the true mean is within a lower and upper bound. To compute the confidence interval, we need to compute the sample mean, $\\bar{x}$, sample standard deviation, $S$, and the number of observations for each classifier, $n$. ***In our specific case, the number of observations should be 5 because we have 5 reported F1 scores from cross-validation.***\n",
        "\n",
        "Then, the confidence interval is computed by\n",
        "    \n",
        "$$\\bar{x} \\pm t \\cdot \\frac{S}{\\sqrt{n}}$$\n",
        "\n",
        "Here, $t$ is the critical value, which we can look up using the provided t-table (https://www.stat.colostate.edu/inmem/gumina/st201/pdf/Utts-Heckard_t-Table.pdf). (Round up the critical value to the second digit below the decimal point) For example, when $n=10$, if we are looking for a 99\\% confidence interval, then the number in the 99\\% confidence column with degrees of freedom of $n-1=9$ would be $t=3.25$. Then, we can plug in all of the statistics into the confidence interval formula and get a range of values for which we are 99\\% confident that the true F1 score of the classifier falls between.\n",
        "\n",
        "For this computation, we should use the unbiased estimator of the variance, which means that the degrees of freedom on the standard deviation calculation must be set. Look in the optional arguments of np.std to learn more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mDdt6GfddPn"
      },
      "outputs": [],
      "source": [
        "def calculate_confidence_interval(cv_f1_scores):\n",
        "    '''\n",
        "    Args:\n",
        "        cv_f1_scores      :   np.array, reported cross-validation F1 scores\n",
        "    Returns:\n",
        "        interval    :   np.array, lower bound and upper bound of the 99% confidence interval\n",
        "    '''\n",
        "    \n",
        "    # TODO: this function should be able to handle two confidence levels of 99% and 80%\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code starts ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7oV-VY03B6S"
      },
      "outputs": [],
      "source": [
        "def test_confidence_intervals():\n",
        "    data = np.array([15.6, 16.2, 22.5, 20.5, 16.4])\n",
        "    result = np.round(calculate_confidence_interval(data), 3)\n",
        "    interval = np.array([11.918, 24.562])\n",
        "    assert (np.array_equal(interval, result))\n",
        "\n",
        "test_confidence_intervals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "586Wom54xlVw",
        "outputId": "df84ceeb-77c2-409b-e098-ffdf31413d09"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_confidence_intervals', answer = grader_serialize(calculate_confidence_interval))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajUkK26YLA1i"
      },
      "source": [
        "## **4.5. Performance Table [6 pts, manually graded]**\n",
        "\n",
        "Repeat the process for two other sets of features and present a performance table (like the one shown below) that compares the F1 scores and confidence intervals of the three sets of features, indicating which one is your chosen best set. Remember that each set should include the 13 features mentioned earlier. As mentioned earlier, submit this table along with the written homework solutions as this is manually graded.\n",
        "\n",
        "---\n",
        "\n",
        "S.No. | Features | Best CCP Alpha | Mean Cross-validation F1 Score | Cross-validation F1 Score Confidence Interval\n",
        "--- | --- | --- | --- | ---\n",
        "1 | Set 1 | | |\n",
        "2 | Set 2 | | |\n",
        "3 | Set 3 | | |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovIZMkN131ok"
      },
      "source": [
        "# **5. Logistic Regression VS. Decision Tree**\n",
        "\n",
        "Let's revisit the Logistic Regression model and compare the performance of it and with Decision Tree on the Diabietes dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEw4D5tpbXR2"
      },
      "source": [
        "## **5.1. Fit the Logistic Regression on Diabetes dataset**\n",
        "\n",
        "Fit a simple logistic regression on the training data using l2 penalty, $\\alpha$ = 0.01, maximum of iterations = 1000, and weight for the regularization consant for the l2 penalty  term is 0.001. \n",
        "You should be rescaling features using MinMaxScaler from sklearn.preprocessing to make sure that the features are properly scaled for learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDAFJTHJbTEv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def fit_and_predict_logistic(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Fit a logistic regression classifier on the training data and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "    \n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "    y_test_pred_logistic = fit_and_predict_logistic(train_df, test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP9Mo0Ams8Yq",
        "outputId": "892f7ad7-4367-4288-9390-c9138ed89f70"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # PennGrader Grading Cell\n",
        "    grader.grade(test_case_id = 'test_fit_and_predict_logistic', answer = y_test_pred_logistic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er95wB7MtDnq"
      },
      "source": [
        "## **5.2. Fit the Logistic Regression on Diabetes dataset**\n",
        "\n",
        "Find the best alpha that has the highest mean value of f1 scores across 5-fold cross-validation. You should use the same min-max scaler and your candidates for `alpha` should be `[0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05]`. Use the same values for `max_iter`, `lambd` and `penalty` as 5.1. You can still use the `cross_val_score` from scikitlearn here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pThjfGFHnVJC",
        "outputId": "9689f002-9110-481d-ab3b-4e1845333b97"
      },
      "outputs": [],
      "source": [
        "import warnings, tqdm\n",
        "\n",
        "#suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def best_alpha_f1_logistic(train_df):\n",
        "    \"\"\"\n",
        "    Returns the learning rate (alpha) with the highest mean cross-validation F1 score along with the \n",
        "    five cross-validation F1 scores corresponding to the alpha(cv_f1_scores).\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "\n",
        "    Returns:\n",
        "        best_alpha: the tuned best ccp alpha value\n",
        "        cv_f1_scores: the five cross-validation F1 scores for the best_alpha\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    ...\n",
        "\n",
        "    #### Student code ends ####    \n",
        "\n",
        "def refit_and_predict_logistic(train_df, test_df, best_alpha):\n",
        "    \"\"\"\n",
        "    Fit a logistic regressor on the training data using the best_alpha value and output the predictions on the\n",
        "    test set.\n",
        "\n",
        "    Args:\n",
        "        train_df\n",
        "        test_df\n",
        "        best_alpha\n",
        "\n",
        "    Returns:\n",
        "        y_test_pred: The predicted values for the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO:\n",
        "    #### Student code starts ####\n",
        "    \n",
        "    ...\n",
        "    #### Student code ends ####\n",
        "\n",
        "if NOTEBOOK:\n",
        "    best_alpha, cv_f1_scores = best_alpha_f1_logistic(train_df)\n",
        "    y_test_pred_refit_logistic = refit_and_predict_logistic(train_df, test_df, best_alpha)\n",
        "    confidence_interval_logistic = calculate_confidence_interval(cv_f1_scores)\n",
        "    print(best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcUhMc8rGDnp",
        "outputId": "178d1ba3-dcc9-4f55-acc8-35c1c19d8f1f"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # PennGrader Grading Cell\n",
        "    grader.grade(test_case_id = 'test_refit_and_predict_logistic_confidence_interval', answer = (y_test_pred_refit_logistic, confidence_interval_logistic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUrum0tJSJ"
      },
      "source": [
        "End of trials, should not be included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZq75hF9DiUZ"
      },
      "source": [
        "## **5.3. Performance Table [6 pts, manually graded]**\n",
        "\n",
        "Using the 3 best features from 4.5, repeat the process with logistic regression, ultimately finding Cross-validation F1 Score Confidence Interval. As mentioned earlier, submit this table along with the written homework solutions as this is manually graded. (Note: although F1 scores are always positive, the 99% confidence interval may appear as one end being negative. Although this is not ideal, it is still possible if the fold number is small or the average F1 value is small.)\n",
        "\n",
        "---\n",
        "\n",
        "S.No. | Features | Best Alpha | Mean Cross-validation F1 Score | Cross-validation F1 Score Confidence Interval\n",
        "--- | --- | --- | --- | ---\n",
        "1 | Set 1 | | |\n",
        "2 | Set 2 | | |\n",
        "3 | Set 3 | | |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPkrjyp5vub"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOT4XbEE6MKF"
      },
      "source": [
        "- Submit the notebook as a `.ipynb` file to the coding portion of the Gradescope submission. This can be done in Google Colab via the `File - Download .ipynb` menu option."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
